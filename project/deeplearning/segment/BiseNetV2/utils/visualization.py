import os
import torch
import numpy as np
import cv2
import logging
import random
import psutil
from torchvision.transforms import ToPILImage
from tqdm import tqdm


def save_segmentation_results(model, data_loader, device, epoch, class_colors, save_dir="visualization", max_samples=10):
    """ä¿å­˜åˆ†å‰²ç»“æœå¯è§†åŒ–"""
    try:
        # logging.info(f"ğŸ¨ [DEBUG] Starting visualization for {epoch}...")
        model.eval()
        vis_dir = os.path.join(save_dir, f"epoch_{epoch}")
        os.makedirs(vis_dir, exist_ok=True)
        # logging.info(f"ğŸ¨ [DEBUG] Created visualization directory: {vis_dir}")

        # è®¡ç®—æ€»æ ·æœ¬æ•°å¹¶éšæœºé‡‡æ ·
        total_samples = len(data_loader.dataset)
        if max_samples < total_samples:
            # éšæœºé€‰æ‹©æ ·æœ¬ç´¢å¼•
            selected_indices = random.sample(range(total_samples), max_samples)
            selected_indices.sort()  # æ’åºä»¥ä¾¿é«˜æ•ˆéå†
            # logging.info(f"ğŸ¨ [DEBUG] Randomly selected {max_samples} samples from {total_samples} total samples")
        else:
            selected_indices = list(range(min(max_samples, total_samples)))
            # logging.info(f"ğŸ¨ [DEBUG] Using first {len(selected_indices)} samples")

        total_saved = 0
        current_sample_idx = 0
        selected_set = set(selected_indices)
        
        with torch.no_grad():
            for idx, (images, labels) in enumerate(data_loader):
                try:
                    # logging.info(f"ğŸ¨ [DEBUG] Processing batch {idx+1}/{len(data_loader)}")
                    
                    # é™åˆ¶æ ·æœ¬æ•°é‡ä»¥é¿å…å†…å­˜é—®é¢˜
                    if total_saved >= max_samples:
                        logging.info(f"ğŸ¨ [DEBUG] Reached max samples limit ({max_samples}), stopping")
                        break
                    
                    # ç§»åŠ¨åˆ°GPUå¹¶é¢„æµ‹
                    images_gpu = images.to(device)
                    # logging.info(f"ğŸ¨ [DEBUG] Images moved to device, shape: {images_gpu.shape}")
                    
                    # è·å–é¢„æµ‹ç»“æœ
                    outputs = model(images_gpu)
                    if isinstance(outputs, tuple):
                        preds = outputs[0]  # ä¸»å¤´è¾“å‡º
                    else:
                        preds = outputs
                    
                    preds = torch.argmax(preds, dim=1).cpu().numpy()
                    # logging.info(f"ğŸ¨ [DEBUG] Predictions computed, shape: {preds.shape}")
                    
                    # ç§»åŠ¨åˆ°CPUå¹¶ç«‹å³æ¸…ç†GPUå˜é‡
                    labels_np = labels.cpu().numpy()
                    images_cpu = images.cpu()
                    
                    # ç«‹å³åˆ é™¤GPU tensor
                    del images_gpu, outputs
                    torch.cuda.empty_cache()
                    # logging.info(f"ğŸ¨ [DEBUG] GPU tensors cleared")

                    # å¤„ç†æ¯ä¸ªæ ·æœ¬
                    batch_size = images_cpu.size(0)
                    for i in range(batch_size):
                        # æ£€æŸ¥å½“å‰æ ·æœ¬æ˜¯å¦è¢«é€‰ä¸­
                        if current_sample_idx not in selected_set:
                            current_sample_idx += 1
                            continue
                            
                        if total_saved >= max_samples:
                            break
                            
                        try:
                            # logging.info(f"ğŸ¨ [DEBUG] Processing selected sample {current_sample_idx} ({total_saved+1}/{max_samples})")
                            
                            # å†…å­˜ç›‘æ§
                            if total_saved % 5 == 0:  # æ¯5ä¸ªæ ·æœ¬è®°å½•ä¸€æ¬¡
                                mem = psutil.virtual_memory()
                                gpu_mem = torch.cuda.memory_allocated() / 1024**3 if torch.cuda.is_available() else 0
                                # logging.info(f"ğŸ§  [VIS-MEMORY] Sample {total_saved}: RAM {mem.percent:.1f}%, GPU {gpu_mem:.1f}GB")
                            
                            # åå½’ä¸€åŒ–å›¾åƒ
                            img_tensor = images_cpu[i].clone()
                            img_tensor = img_tensor * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
                            img_tensor = torch.clamp(img_tensor, 0, 1)
                            
                            # è½¬æ¢ä¸º PIL å›¾åƒå†è½¬ BGR
                            to_pil = ToPILImage()
                            img = to_pil(img_tensor)
                            img_bgr = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)
                            
                            # ç«‹å³åˆ é™¤ä¸´æ—¶tensor
                            del img_tensor, img
                            # logging.info(f"ğŸ¨ [DEBUG] Image converted, shape: {img_bgr.shape}")

                            # æ„é€ å¯è§†åŒ– label å’Œ pred å½©å›¾
                            h, w = labels_np[i].shape
                            label_color = np.zeros((h, w, 3), dtype=np.uint8)
                            pred_color = np.zeros((h, w, 3), dtype=np.uint8)

                            for class_id, color in class_colors.items():
                                if class_id < len(class_colors):  # å®‰å…¨æ£€æŸ¥
                                    label_color[labels_np[i] == class_id] = color
                                    pred_color[preds[i] == class_id] = color
                            
                            # logging.info(f"ğŸ¨ [DEBUG] Color maps created")

                            # ä¿å­˜ä¸‰å›¾æ‹¼æ¥
                            vis_img = np.concatenate([
                                img_bgr,
                                label_color,
                                pred_color
                            ], axis=1)
                            
                            save_path = os.path.join(vis_dir, f"sample_{total_saved:04d}.png")
                            # logging.info(f"ğŸ¨ [DEBUG] Saving to: {save_path}")
                            
                            success = cv2.imwrite(save_path, vis_img)
                            if success:
                                total_saved += 1
                                # logging.info(f"ğŸ¨ [DEBUG] Sample saved successfully ({total_saved}/{max_samples})")
                            else:
                                logging.error(f"ğŸ¨ [ERROR] Failed to save image: {save_path}")
                            
                            # æ¸…ç†numpyæ•°ç»„
                            del img_bgr, label_color, pred_color, vis_img
                            
                            # å¼ºåˆ¶åƒåœ¾å›æ”¶
                            if total_saved % 10 == 0:
                                import gc
                                gc.collect()
                            
                        except Exception as e:
                            logging.error(f"ğŸ¨ [ERROR] Failed to process sample {current_sample_idx}: {str(e)}")
                        finally:
                            current_sample_idx += 1
                                

                    
                    # æ¸…ç†æ‰¹æ¬¡æ•°æ®
                    del labels_np, images_cpu, preds
                    # logging.info(f"ğŸ¨ [DEBUG] Batch {idx+1} completed, total saved: {total_saved}")
                    
                    # å¦‚æœå·²ç»ä¿å­˜è¶³å¤Ÿçš„æ ·æœ¬ï¼Œæå‰é€€å‡º
                    if total_saved >= max_samples:
                        # logging.info(f"ğŸ¨ [DEBUG] Reached target sample count, stopping early")
                        break
                    
                except Exception as e:
                    # logging.error(f"ğŸ¨ [ERROR] Failed to process batch {idx}: {str(e)}")
                    # æ¸…ç†å¯èƒ½çš„GPUå†…å­˜
                    torch.cuda.empty_cache()
                    continue
        
        # logging.info(f"ğŸ¨ [DEBUG] Visualization completed, total saved: {total_saved}")
        return total_saved
        
    except Exception as e:
        # logging.error(f"ğŸ¨ [ERROR] Visualization function failed: {str(e)}")
        import traceback
        logging.error(f"ğŸ¨ [ERROR] Traceback: {traceback.format_exc()}")
        return 0
    
    finally:
        # ç¡®ä¿æ¸…ç†èµ„æº
        import gc
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        logging.info(f"ğŸ¨ [DEBUG] Visualization function cleanup completed")