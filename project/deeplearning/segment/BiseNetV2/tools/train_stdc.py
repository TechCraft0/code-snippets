import os
import sys
import math
import torch
import logging
import argparse
import torch.nn as nn
from tqdm import tqdm
import torch.nn.functional as F
import psutil
import gc
from memory_profiler import profile
import tracemalloc

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from cfg.config_stdc import *

# æ—¥å¿—æ§åˆ¶é…ç½®
DEBUG_CONFIG = {
    'enable_detailed_logs': False,  # è¯¦ç»†æ—¥å¿—å¼€å…³
    'enable_memory_logs': False,  # å†…å­˜ç›‘æ§æ—¥å¿—å¼€å…³
    'enable_validation_logs': False,  # éªŒè¯è¿‡ç¨‹è¯¦ç»†æ—¥å¿—
    'enable_visualization_logs': False,  # å¯è§†åŒ–è¿‡ç¨‹è¯¦ç»†æ—¥å¿—
}
from torch import optim
from tqdm import tqdm
from data.data_load import *
from model.bisenetv2_stdc import BisenetV2STDC
from utils.common import create_experiment_dirs
from torch.utils.data import Dataset, DataLoader
from tools.val import validate
from utils.visualization import save_segmentation_results
from utils.plot_curves import plot_training_curves
from model.common import DetailAggregateLoss


def log_memory_usage(stage=""):
    """è®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    try:
        # ç³»ç»Ÿå†…å­˜
        mem = psutil.virtual_memory()
        swap = psutil.swap_memory()

        # GPUå†…å­˜
        gpu_allocated = 0
        gpu_reserved = 0
        if torch.cuda.is_available():
            gpu_allocated = torch.cuda.memory_allocated() / 1024 ** 3
            gpu_reserved = torch.cuda.memory_reserved() / 1024 ** 3

        logging.info(
            f"ğŸ§  [MEMORY] {stage} | RAM: {mem.percent:.1f}% ({mem.used / 1024 ** 3:.1f}GB/{mem.total / 1024 ** 3:.1f}GB) | "
            f"Swap: {swap.percent:.1f}% ({swap.used / 1024 ** 3:.1f}GB) | "
            f"GPU: {gpu_allocated:.1f}GB allocated, {gpu_reserved:.1f}GB reserved")

        # å†…å­˜å¼‚å¸¸æ£€æµ‹
        if mem.percent > 85:
            logging.warning(f"âš ï¸ [MEMORY] High RAM usage: {mem.percent:.1f}%")
        if swap.percent > 50:
            logging.warning(f"âš ï¸ [MEMORY] High swap usage: {swap.percent:.1f}%")
        if gpu_allocated > 12:  # RTX 4060 Ti 16GBçš„80%
            logging.warning(f"âš ï¸ [MEMORY] High GPU usage: {gpu_allocated:.1f}GB")

    except Exception as e:
        logging.error(f"ğŸ§  [MEMORY] Failed to log memory usage: {str(e)}")


def train(
        model,
        train_loader,
        val_loader,
        test_loader,
        optimizer,
        scheduler,
        loss_function,
        device,
        model_save_path,
        visualization_save_path,
        class_colors,
        max_iter,
        start_iter=0,
        resume_metrics=None,
):
    model.train()
    train_losses = []
    iter_count = start_iter

    # Initialize edge loss function once
    edge_loss_fn = DetailAggregateLoss().to(device)

    # Clear GPU cache
    torch.cuda.empty_cache()

    # è®°å½•è®­ç»ƒæŒ‡æ ‡ï¼ˆæ”¯æŒæ–­ç‚¹ç»­è®­ï¼‰
    if resume_metrics is not None:
        metrics_history = resume_metrics
        print(f"ğŸ“ˆ Resumed metrics history with {len(metrics_history['train_total_loss'])} training records")
    else:
        metrics_history = {
            'train_total_loss': [],
            'train_seg_loss': [],
            'train_edge_loss': [],
            'train_main_loss': [],
            'train_aux0_loss': [],
            'train_aux1_loss': [],
            'train_aux2_loss': [],
            'train_aux3_loss': [],
            'train_iters': [],
            'learning_rate': [],
            'val_loss': [],
            'val_miou': [],
            'val_pixel_acc': [],
            'val_iters': [],
            'test_loss': [],
            'test_miou': [],
            'test_pixel_acc': [],
            'test_iters': []
        }

    # å¯åŠ¨å†…å­˜è·Ÿè¸ª
    tracemalloc.start()
    log_memory_usage("Training Start")

    # Simple progress tracking without tqdm
    print(f"ğŸš€ Training started: {start_iter}/{max_iter} iterations")

    while iter_count < max_iter:
        for images, labels in train_loader:
            iter_count += 1
            if iter_count >= max_iter:
                break

            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()

            main_head, aux_heads, x1 = model(images)
            bce_loss, dice_loss = edge_loss_fn(x1, labels)
            edge_loss = bce_loss + dice_loss
            losses = [loss_function(main_head, labels)]
            for head in aux_heads:
                losses.append(loss_function(head, labels))
            seg_loss = torch.stack(losses).sum()
            loss = seg_loss + edge_loss

            loss.backward()
            optimizer.step()
            scheduler.step()

            train_losses.append(loss.item())

            # Calculate metrics for logging
            current_lr = optimizer.param_groups[0]['lr']
            avg_loss = sum(train_losses[-50:]) / min(50, len(train_losses))

            # è®­ç»ƒæŸå¤±æ—¥å¿— - ä»…æ˜¾ç¤ºå…³é”®ä¿¡æ¯
            if iter_count % 50 == 0:
                progress = iter_count / max_iter
                bar_length = 20
                filled_length = int(bar_length * progress)
                bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
                percentage = progress * 100

                # è¯¦ç»†çš„è®­ç»ƒæŸå¤±æ—¥å¿—
                loss_details = " | ".join([f"H{i}: {l.item():.4f}" for i, l in enumerate(losses)])
                print(
                    f"ğŸ“Š [{bar}] {percentage:5.1f}% Iter {iter_count:>6} | Total: {loss.item():.4f} | Seg: {seg_loss.item():.4f} | Edge: {edge_loss.item():.4f} | {loss_details} | LR: {current_lr:.2e}")

                # è®°å½•è®­ç»ƒæŸå¤±åˆ†è§£
                metrics_history['train_total_loss'].append(loss.item())
                metrics_history['train_seg_loss'].append(seg_loss.item())
                metrics_history['train_edge_loss'].append(edge_loss.item())
                metrics_history['train_main_loss'].append(losses[0].item())
                for i, aux_loss in enumerate(losses[1:]):
                    if i < 4:
                        metrics_history[f'train_aux{i}_loss'].append(aux_loss.item())
                metrics_history['train_iters'].append(iter_count)
                metrics_history['learning_rate'].append(current_lr)

                # å†…å­˜ç›‘æ§
                if DEBUG_CONFIG['enable_memory_logs'] and iter_count % 200 == 0:
                    log_memory_usage(f"Iter {iter_count}")

            # Validation
            if (
                    iter_count % TRAIN_PARAMS.get("val_interval", 1000) == 0
                    or iter_count == max_iter
            ):
                try:
                    # æ¸…ç†GPUç¼“å­˜å’Œå†…å­˜
                    gc.collect()
                    torch.cuda.empty_cache()
                    if DEBUG_CONFIG['enable_memory_logs']:
                        log_memory_usage(f"Before Validation Iter {iter_count}")

                    val_loss, miou, pixel_acc = validate(
                        model,
                        val_loader,
                        loss_function,
                        device,
                        MODEL_PARAMS["num_classes"],
                        stdc=True
                    )

                    # ç®€åŒ–çš„éªŒè¯ç»“æœè¾“å‡º
                    print(
                        f"ğŸ” Validation | Iter {iter_count} | Loss: {val_loss:.4f} | mIoU: {miou:.4f} | PixelAcc: {pixel_acc:.4f}")

                    # è®°å½•éªŒè¯æŒ‡æ ‡
                    metrics_history['val_loss'].append(val_loss)
                    metrics_history['val_miou'].append(miou)
                    metrics_history['val_pixel_acc'].append(pixel_acc)
                    metrics_history['val_iters'].append(iter_count)

                except Exception as e:
                    error_msg = f"âš ï¸ Validation failed at iter {iter_count}: {str(e)}"
                    print(error_msg)
                    logging.error(error_msg)
                    import traceback
                    logging.error(f"âš ï¸ Validation traceback: {traceback.format_exc()}")

                model.train()

            # Test evaluation and visualization
            if (
                    iter_count % TRAIN_PARAMS.get("test_interval", 200) == 0
                    or iter_count == max_iter
            ):
                try:
                    # æ¸…ç†GPUç¼“å­˜
                    torch.cuda.empty_cache()
                    if DEBUG_CONFIG['enable_memory_logs']:
                        log_memory_usage(f"Before Test Iter {iter_count}")

                    test_loss, test_miou, test_pixel_acc = validate(
                        model,
                        test_loader,
                        loss_function,
                        device,
                        MODEL_PARAMS["num_classes"],
                        stdc=True,
                    )

                    # ç®€åŒ–çš„æµ‹è¯•ç»“æœè¾“å‡º
                    print(
                        f"ğŸ† Test | Iter {iter_count} | Loss: {test_loss:.4f} | mIoU: {test_miou:.4f} | PixelAcc: {test_pixel_acc:.4f}")

                    # è®°å½•æµ‹è¯•æŒ‡æ ‡
                    metrics_history['test_loss'].append(test_loss)
                    metrics_history['test_miou'].append(test_miou)
                    metrics_history['test_pixel_acc'].append(test_pixel_acc)
                    metrics_history['test_iters'].append(iter_count)

                    # å¯è§†åŒ–ç”Ÿæˆ
                    if TRAIN_PARAMS.get("enable_visualization", True):
                        torch.cuda.empty_cache()
                        if DEBUG_CONFIG['enable_memory_logs']:
                            log_memory_usage(f"Before Visualization Iter {iter_count}")

                        max_samples = TRAIN_PARAMS.get("max_vis_samples", 5)
                        saved_count = save_segmentation_results(model, test_loader, device, f"iter_{iter_count}",
                                                                class_colors, visualization_save_path,
                                                                max_samples=max_samples)
                        print(f"ğŸ¨ Visualizations saved: {saved_count} samples")

                except Exception as e:
                    error_msg = f"âš ï¸ Test evaluation/visualization failed at iter {iter_count}: {str(e)}"
                    print(error_msg)
                    logging.error(error_msg)
                    import traceback
                    logging.error(f"âš ï¸ Test traceback: {traceback.format_exc()}")

                model.train()

            # Save checkpoint
            if iter_count % TRAIN_PARAMS.get("checkpoint_interval", 1000) == 0 or iter_count == max_iter:
                try:
                    logging.info(f"ğŸ’¾ [DEBUG] Starting checkpoint save at iter {iter_count}...")
                    checkpoint_data = {
                        'model_state_dict': model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'scheduler_state_dict': scheduler.state_dict(),
                        'iter_count': iter_count,
                        'metrics_history': metrics_history
                    }

                    checkpoint_path = os.path.join(model_save_path, f'model_iter_{iter_count}.pth')
                    logging.info(f"ğŸ’¾ [DEBUG] Saving checkpoint to: {checkpoint_path}")
                    torch.save(checkpoint_data, checkpoint_path)

                    save_msg = f"ğŸ’¾ Checkpoint saved at iter {iter_count}"
                    print(save_msg)
                    logging.info(save_msg)
                    logging.info(f"ğŸ’¾ [DEBUG] Checkpoint save completed successfully")
                except Exception as e:
                    error_msg = f"âš ï¸ Checkpoint save failed at iter {iter_count}: {str(e)}"
                    print(error_msg)
                    logging.error(error_msg)
                    import traceback
                    logging.error(f"âš ï¸ Checkpoint save traceback: {traceback.format_exc()}")

    print(f"âœ… Training completed! Final iteration: {iter_count}/{max_iter}")
    logging.info("âœ… Training completed!")

    # æœ€ç»ˆå†…å­˜æŠ¥å‘Š
    log_memory_usage("Training Completed")

    # æ˜¾ç¤ºå†…å­˜å¢é•¿æœ€å¤§çš„å‰10ä¸ªä½ç½®
    try:
        current, peak = tracemalloc.get_traced_memory()
        logging.info(f"ğŸ§  [MEMORY] Peak memory usage: {peak / 1024 ** 2:.1f} MB")

        snapshot = tracemalloc.take_snapshot()
        top_stats = snapshot.statistics('lineno')
        logging.info("ğŸ§  [MEMORY] Top 5 memory allocations:")
        for index, stat in enumerate(top_stats[:5], 1):
            logging.info(f"ğŸ§  [MEMORY] #{index}: {stat}")

        tracemalloc.stop()
    except Exception as e:
        logging.error(f"ğŸ§  [MEMORY] Failed to get memory trace: {str(e)}")

    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    try:
        logging.info("ğŸ“ˆ [DEBUG] Starting to plot training curves...")
        print("ğŸ“ˆ Plotting training curves...")
        plot_training_curves(metrics_history, visualization_save_path)
        print(f"ğŸ“ˆ Training curves saved to {visualization_save_path}/training_curves.png")
        logging.info(f"ğŸ“ˆ Training curves saved to {visualization_save_path}/training_curves.png")
        logging.info("ğŸ“ˆ [DEBUG] Training curves plotting completed successfully")
    except Exception as e:
        error_msg = f"âš ï¸ Training curves plotting failed: {str(e)}"
        print(error_msg)
        logging.error(error_msg)
        import traceback
        logging.error(f"âš ï¸ Training curves traceback: {traceback.format_exc()}")


@profile
def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='BiseNetV2 Training')
    parser.add_argument('--resume', action='store_true', help='Resume training from checkpoint')
    parser.add_argument('--no-resume', action='store_true', help='Force start training from scratch')
    parser.add_argument('--debug', action='store_true', help='Enable detailed debug logs')
    parser.add_argument('--debug-memory', action='store_true', help='Enable memory monitoring logs')
    parser.add_argument('--debug-val', action='store_true', help='Enable validation debug logs')
    parser.add_argument('--debug-vis', action='store_true', help='Enable visualization debug logs')
    args = parser.parse_args()

    # æ›´æ–°æ—¥å¿—é…ç½®
    if args.debug:
        DEBUG_CONFIG.update({
            'enable_detailed_logs': True,
            'enable_memory_logs': True,
            'enable_validation_logs': True,
            'enable_visualization_logs': True,
        })
    else:
        if args.debug_memory:
            DEBUG_CONFIG['enable_memory_logs'] = True
        if args.debug_val:
            DEBUG_CONFIG['enable_validation_logs'] = True
        if args.debug_vis:
            DEBUG_CONFIG['enable_visualization_logs'] = True

    # get data path
    train_img_path = PATHS['train_img_path']
    train_label_path = PATHS['train_label_path']
    val_img_path = PATHS['val_img_path']
    val_label_path = PATHS['val_label_path']
    test_img_path = PATHS['test_img_path']
    test_label_path = PATHS['test_label_path']

    # create train result path or find existing one for resume
    enable_resume = not args.no_resume and (args.resume or TRAIN_PARAMS.get("auto_resume", True))

    if enable_resume:
        # æŸ¥æ‰¾æœ€æ–°çš„å®éªŒç›®å½•
        root_dir = PATHS["root_dir"]
        if os.path.exists(root_dir):
            exp_dirs = [d for d in os.listdir(root_dir) if
                        d.startswith(PATHS["name"] + "_") and os.path.isdir(os.path.join(root_dir, d))]
            if exp_dirs:
                latest_exp = sorted(exp_dirs, key=lambda x: int(x.split("_")[-1]))[-1]
                exp_path = os.path.join(root_dir, latest_exp)
                model_save_path = os.path.join(exp_path, PATHS["save_dir"])
                logs_save_path = os.path.join(exp_path, PATHS["log_dir"])
                visualization_save_path = os.path.join(exp_path, PATHS["visualization_dir"])
                print(f"ğŸ”„ Found existing experiment: {latest_exp}")
            else:
                model_save_path, logs_save_path, visualization_save_path = create_experiment_dirs(
                    PATHS["root_dir"], PATHS["name"], PATHS["save_dir"], PATHS["log_dir"], PATHS["visualization_dir"])
        else:
            model_save_path, logs_save_path, visualization_save_path = create_experiment_dirs(
                PATHS["root_dir"], PATHS["name"], PATHS["save_dir"], PATHS["log_dir"], PATHS["visualization_dir"])
    else:
        model_save_path, logs_save_path, visualization_save_path = create_experiment_dirs(
            PATHS["root_dir"], PATHS["name"], PATHS["save_dir"], PATHS["log_dir"], PATHS["visualization_dir"])

    # setup logging with emoji support
    log_file = os.path.join(logs_save_path, 'train.log')
    logging.basicConfig(
        level=logging.INFO,  # æ”¹ä¸ºDEBUGçº§åˆ«ä»¥è·å–æ›´å¤šä¿¡æ¯
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ],
        force=True  # å¼ºåˆ¶é‡æ–°é…ç½®logging
    )

    # æ·»åŠ å†…å­˜å’ŒGPUä¿¡æ¯ç›‘æ§
    if torch.cuda.is_available():
        logging.info(f"ğŸ”§ [DEBUG] GPU: {torch.cuda.get_device_name()}")
        logging.info(f"ğŸ”§ [DEBUG] GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.1f} GB")

    logging.info(
        f"ğŸš€ Training started! ğŸ“ Model save path: {model_save_path} ğŸ“Š Logs save path: {logs_save_path} ğŸ¨ Visualization save path: {visualization_save_path}"
    )

    # get class names and colors
    classes_name_color_pairs = CLASS_NAME
    # Convert to BGR format for visualization
    class_colors = {cls_id: tuple(cls_info['color'][::-1]) for cls_id, cls_info in classes_name_color_pairs.items()}

    # setting max iteration
    max_iter = TRAIN_PARAMS["total_iters"]

    # åé¢å¯ä»¥ä½¿ç”¨è¿™ä¸ªå­—å…¸ï¼Œæ¯”å¦‚ç»˜åˆ¶çš„æ—¶å€™ï¼š
    for cls_id, cls_info in classes_name_color_pairs.items():
        logging.info(f"ğŸŒˆ Class {cls_id}: {cls_info['name']}, color: {cls_info['color']}")

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    train_set = LoadImageAndLabels(
        train_img_path,
        train_label_path,
        TRAIN_PARAMS["input_size"],
        TRAIN_PARAMS["mode"],
        TRAIN_PARAMS["crop_size"])
    train_loader = DataLoader(
        train_set,
        batch_size=TRAIN_PARAMS["batch_size"],
        shuffle=True,
        num_workers=TRAIN_PARAMS["num_workers"],
        pin_memory=TRAIN_PARAMS["pin_memory"],
        drop_last=TRAIN_PARAMS["drop_last"])

    val_set = LoadImageAndLabels(
        val_img_path,
        val_label_path,
        TRAIN_PARAMS["input_size"],
        mode="val")
    val_loader = DataLoader(
        val_set,
        batch_size=TRAIN_PARAMS["batch_size"],
        shuffle=False,
        num_workers=TRAIN_PARAMS["num_workers"],
        pin_memory=TRAIN_PARAMS["pin_memory"],
        drop_last=False)

    test_set = LoadImageAndLabels(
        test_img_path,
        test_label_path,
        TRAIN_PARAMS["input_size"],
        mode="val")
    test_loader = DataLoader(
        test_set,
        batch_size=TRAIN_PARAMS["batch_size"],
        shuffle=False,
        num_workers=TRAIN_PARAMS["num_workers"],
        pin_memory=TRAIN_PARAMS["pin_memory"],
        drop_last=False)

    # model config
    model = BisenetV2STDC(
        in_channels=MODEL_PARAMS["in_channels"],
        out_channels=MODEL_PARAMS["out_channels"],
        n_classes=MODEL_PARAMS["num_classes"]).to(device)

    # loss function
    if LOSS_PARAMS["type"] == "ce":
        loss_function = nn.CrossEntropyLoss()
    else:
        loss_function = nn.CrossEntropyLoss()

    if OPTIMIZER_PARAMS == "sgd":
        optimizer = optim.SGD(model.parameters(), lr=TRAIN_PARAMS["lr"], momentum=0.9, weight_decay=5e-4)
    elif OPTIMIZER_PARAMS == "adam":
        optimizer = optim.Adam(model.parameters(), lr=TRAIN_PARAMS["lr"], weight_decay=5e-4)
    elif OPTIMIZER_PARAMS == "adamw":
        optimizer = optim.AdamW(model.parameters(), lr=TRAIN_PARAMS["lr"], weight_decay=5e-4)
    else:
        raise ValueError(f"ğŸš¨ Unsupported optimizer type: {LOSS_PARAMS['type']}")

    # learning rate scheduler
    lr_scheduler_type = LR_SCHEDULER_PARAMS["type"]
    if lr_scheduler_type == "linear":
        scheduler = optim.lr_scheduler.LambdaLR(
            optimizer,
            lr_lambda=lambda it: 1.0 - it / LR_SCHEDULER_PARAMS["total_iters"]
        )
    elif lr_scheduler_type == "step":
        scheduler = optim.lr_scheduler.StepLR(
            optimizer,
            step_size=LR_SCHEDULER_PARAMS["step_size"],
            gamma=LR_SCHEDULER_PARAMS["gamma"]
        )
    elif lr_scheduler_type == "poly":
        def poly_decay(it):
            return (1 - it / LR_SCHEDULER_PARAMS["total_iters"]) ** LR_SCHEDULER_PARAMS["power"]

        scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=poly_decay)
    elif lr_scheduler_type == "cos":
        scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=LR_SCHEDULER_PARAMS["total_iters"]
        )
    elif lr_scheduler_type == "warmcos":
        def warm_cosine(it):
            warmup_iters = LR_SCHEDULER_PARAMS["warmup_iters"]
            total_iters = LR_SCHEDULER_PARAMS["total_iters"]
            if it < warmup_iters:
                return it / warmup_iters
            else:
                return 0.5 * (1 + math.cos(math.pi * (it - warmup_iters) / (total_iters - warmup_iters)))

        scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warm_cosine)
    else:
        raise ValueError(f"ğŸš¨ Unsupported LR scheduler type: {lr_scheduler_type}")

    # checkpoint resume
    iter_count = 0
    resume_metrics = None

    if enable_resume and os.path.exists(model_save_path) and os.listdir(model_save_path):
        checkpoint_files = [f for f in os.listdir(model_save_path) if
                            f.startswith("model_iter_") and f.endswith(".pth")]
        if checkpoint_files:
            latest_ckpt = sorted(checkpoint_files, key=lambda x: int(x.split("_")[-1].split(".")[0]))[-1]
            ckpt_path = os.path.join(model_save_path, latest_ckpt)
            try:
                checkpoint = torch.load(ckpt_path, map_location=device)
                model.load_state_dict(checkpoint['model_state_dict'])
                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                iter_count = checkpoint['iter_count']
                # æ¢å¤æŒ‡æ ‡å†å²ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if 'metrics_history' in checkpoint:
                    resume_metrics = checkpoint['metrics_history']
                logging.info(f"ğŸ”„ Resumed from checkpoint: {latest_ckpt}, iter: {iter_count}")
                print(f"ğŸ”„ Resumed training from iteration {iter_count}")
                # Clear GPU memory after loading checkpoint
                # del checkpoint
                # torch.cuda.empty_cache()
                # torch.cuda.synchronize()
            except Exception as e:
                logging.error(f"âš ï¸ Failed to load checkpoint {latest_ckpt}: {str(e)}")
                logging.info("ğŸ†• Starting training from scratch")
                iter_count = 0
        else:
            logging.info("ğŸ†• No checkpoints found, starting training from scratch")
    else:
        if args.no_resume:
            logging.info("ğŸ†• Force starting training from scratch (--no-resume)")
        elif not enable_resume:
            logging.info("ğŸ†• Auto-resume disabled, starting training from scratch")
        else:
            logging.info("ğŸ†• Starting training from scratch")

    # Start training
    try:
        logging.info("ğŸš€ [DEBUG] Starting training function...")
        train(
            model,
            train_loader,
            val_loader,
            test_loader,
            optimizer,
            scheduler,
            loss_function,
            device,
            model_save_path,
            visualization_save_path,
            class_colors,
            max_iter,
            iter_count,
            resume_metrics,
        )
        logging.info("âœ… [DEBUG] Training function completed successfully")
    except Exception as e:
        error_msg = f"ğŸ’¥ Training failed: {str(e)}"
        print(error_msg)
        logging.error(error_msg)
        import traceback
        logging.error(f"ğŸ’¥ Training traceback: {traceback.format_exc()}")
        raise


if __name__ == '__main__':
    main()